{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Team Members"
      ],
      "metadata": {
        "id": "ARS0SPHj8_oQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Chintalapudi Abhiroop - AI20BTECH11005\n",
        "*   Nelakuditi Rahul Naga - AI20BTECH11029\n",
        "*   Vaddamani Saketh - CS20BTECH11054\n",
        "\n"
      ],
      "metadata": {
        "id": "iKR0OEjj9D30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "Jkg09LtY9Tat"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "oZ4n51YGewas"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Function to pre-process the input sentence by lower-casing the text and removing punctuations\n",
        "def pre_process(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    new_sentence = sentence.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "    return new_sentence\n",
        "\n",
        "# Function to compute the n-gram of an input sentence\n",
        "def n_gram(sentence,n):\n",
        "    tokens = sentence.split()\n",
        "    n_gram = []\n",
        "    for start_ind in range(len(tokens) - n + 1):\n",
        "      n_gram.append(\" \".join(tokens[start_ind:start_ind + n]))\n",
        "    return n_gram\n",
        "\n",
        "# Function to calculate the BLEU score metric between two input sentences\n",
        "def BLEU_score(x,y):\n",
        "  x = pre_process(x) # x -> Prediction\n",
        "  y = pre_process(y) # y -> Ground Truth\n",
        "  N = 4\n",
        "  BP = 1\n",
        "  w_n = 1/N\n",
        "  summation = 0\n",
        "  for n in range(1,N+1):\n",
        "    n_gram_x = n_gram(x,n)\n",
        "    n_gram_y = n_gram(y,n)\n",
        "    n_gram_count_x = dict(Counter(n_gram_x))\n",
        "    n_gram_count_y = dict(Counter(n_gram_y))\n",
        "    numerator = 0\n",
        "    for n_x in list(n_gram_count_x.keys()):\n",
        "      if n_x in list(n_gram_count_y.keys()):\n",
        "        numerator += min(n_gram_count_x[n_x],n_gram_count_y[n_x])\n",
        "    p_n = numerator/sum(n_gram_count_x.values())\n",
        "    if p_n == 0:\n",
        "      return 0\n",
        "    summation += (w_n * math.log(p_n))\n",
        "  return BP * math.exp(summation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2"
      ],
      "metadata": {
        "id": "J6ZTusKO9WLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"The boys were playing happily on the ground.\"\n",
        "y = \"The boys were playing football on the field.\"\n",
        "\n",
        "print(BLEU_score(x,y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix0CJbqUzYmf",
        "outputId": "82fe87ff-8ee4-46a6-9508-728d277ce2d4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4111336169005197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3"
      ],
      "metadata": {
        "id": "5BczJepf9XyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " By using the **minimum** operator in the numerator we are truncating the count of each n-gram in sentence x (prediction), if necessary, to not exceed the count observed in the sentence y (ground truth) for that particular n-gram. This helps us to tackle the problem of inflation in precision caused by the repetition (or) over-generation of the same n-gram (present in both sentences x and y) multiple times ."
      ],
      "metadata": {
        "id": "b3YZpoFt9Z9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4"
      ],
      "metadata": {
        "id": "Hqmszpmi9Zk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_pairs = [(\"The sun never sets on the British Emipre.\",\"The sun never sets on the British Kingdom.\"),\n",
        "                  (\"College hostels are generally very unhygenic.\",\"College hostels are generally very unclean.\"),\n",
        "                  (\"What are you up to these days?\",\"What are you up to nowadays?\"),\n",
        "                  (\"The security is going to catch us anytime!\",\"The security is going to bust us any moment!\"),\n",
        "                  (\"What are you waiting for? Let us get out of here!\",\"Who are you looking for? Let us run out of here!\")]\n",
        "\n",
        "for i in range(len(sentence_pairs)):\n",
        "  sentence_pair = sentence_pairs[i]\n",
        "  print(f'BLEU Score for Sentence pair {i+1} = {BLEU_score(sentence_pair[0],sentence_pair[1])}')"
      ],
      "metadata": {
        "id": "s3jgyjQux32H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37680dc-5c39-47d5-f431-becbaa232902"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score for Sentence pair 1 = 0.8408964152537145\n",
            "BLEU Score for Sentence pair 2 = 0.7598356856515925\n",
            "BLEU Score for Sentence pair 3 = 0.6147881529512643\n",
            "BLEU Score for Sentence pair 4 = 0.5410822690539396\n",
            "BLEU Score for Sentence pair 5 = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The potential disadvantages of using BLEU score are as follows :\n",
        "\n",
        "* It does not take into account the usage of synonyms in the predicted sentence.\n",
        "* Even small variations in n-grams of the predicted sentence might lead to considerable variations in BLEU score, including it becoming zero.\n",
        "* Different n-grams are penalized in the same manner by the BLEU score even though all of them might not contribute to the meaning of a sentence equally.\n",
        "* The sequential order of n-grams in a predicted sentence is not considered while calculating the BLEU score, but changing the order might completely distort the meaning of a sentence.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Ftr-kE9_hyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "rxXmsAa49oXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [BLEU - Papineni et al.](https://aclanthology.org/P02-1040/)\n",
        "* [Towards Data Science Article about BLEU Score](https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b)\n",
        "\n"
      ],
      "metadata": {
        "id": "xY63IOe_9qjt"
      }
    }
  ]
}